{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Lc_qiCCP1u7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1IUY-sEKAAT"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from itertools import combinations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = [\n",
        "        # \"/content/sample_dataset.csv\",\n",
        "        # \"/content/drive/MyDrive/Big Data/datasets/retail.csv\",\n",
        "        # \"/content/drive/MyDrive/Big Data/datasets/chess.csv\",\n",
        "        \"/content/drive/MyDrive/Big Data/datasets/mushroom.csv\",\n",
        "        # \"/content/drive/MyDrive/Big Data/datasets/T10I4D100K.csv\",\n",
        "        # \"/content/drive/MyDrive/Big Data/datasets/pumsb.csv\",\n",
        "]\n",
        "min_support_ratios = [0.5, 0.4, 0.3, 0.2]\n",
        "min_confidence = 0.3"
      ],
      "metadata": {
        "id": "TgrQx24ZbKW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apriori Linear"
      ],
      "metadata": {
        "id": "v469VQSlf7ZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility Functions"
      ],
      "metadata": {
        "id": "Nq8wQRGqgMh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load transaction data from a CSV file."
      ],
      "metadata": {
        "id": "vjaM_0HGdyzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_data(filename):\n",
        "#     df = pd.read_csv(filename, header=None)\n",
        "#     transactions = df.apply(lambda row: [item for item in row if not pd.isna(item)], axis=1)\n",
        "#     transactions = [set(transaction) for transaction in transactions]\n",
        "#     return transactions\n",
        "\n",
        "def load_data(filename):\n",
        "    df = pd.read_csv(filename, header=None)\n",
        "    transactions = df.apply(lambda row: [item for item in row if not pd.isna(item) and item != \"\"], axis=1)\n",
        "    transactions = [set(transaction) for transaction in transactions]\n",
        "    return transactions"
      ],
      "metadata": {
        "id": "w2_ZrPTWbKQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate the 1-itemsets from the transactions."
      ],
      "metadata": {
        "id": "tOi3cUk6cHIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_1_itemsets(transactions):\n",
        "    item_counts = {}\n",
        "    for transaction in transactions:\n",
        "        for item in transaction:\n",
        "            item_counts[frozenset([item])] = item_counts.get(frozenset([item]), 0) + 1\n",
        "    return item_counts"
      ],
      "metadata": {
        "id": "LA4Et0S_cCyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate all frequent itemsets."
      ],
      "metadata": {
        "id": "cmQS3e89dR7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Utility Function : Generate candidate (k-itemsets) from the previous itemsets."
      ],
      "metadata": {
        "id": "Ulne8-oadYPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def generate_candidates(prev_itemsets, k):\n",
        "#     candidates = set()\n",
        "#     itemsets = list(prev_itemsets)\n",
        "#     for i in range(len(itemsets)):\n",
        "#         for j in range(i + 1, len(itemsets)):\n",
        "#             union_set = itemsets[i] | itemsets[j]\n",
        "#             if len(union_set) == k:\n",
        "#                 candidates.add(union_set)\n",
        "#     return candidates\n",
        "\n",
        "def generate_candidates(prev_itemsets, k):\n",
        "    candidates = set()\n",
        "    itemsets = list(prev_itemsets)\n",
        "    for i in range(len(itemsets)):\n",
        "        for j in range(i + 1, len(itemsets)):\n",
        "            union_set = itemsets[i] | itemsets[j]\n",
        "            # Check subset conditions inefficiently\n",
        "            if len(union_set) == k and not any(union_set.issubset(other) for other in candidates):\n",
        "                candidates.add(union_set)\n",
        "    return candidates\n"
      ],
      "metadata": {
        "id": "Ga8AaDOuco1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Utility Function : Filter candidates by support count."
      ],
      "metadata": {
        "id": "gayB-ae3dciL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def filter_candidates(transactions, candidates, min_support):\n",
        "#     candidate_counts = {candidate: 0 for candidate in candidates}\n",
        "#     for transaction in transactions:\n",
        "#         for candidate in candidates:\n",
        "#             if candidate.issubset(transaction):\n",
        "#                 candidate_counts[candidate] += 1\n",
        "#     return {itemset: count for itemset, count in candidate_counts.items() if count >= min_support}\n",
        "\n",
        "def filter_candidates(transactions, candidates, min_support):\n",
        "    candidate_counts = {candidate: 0 for candidate in candidates}\n",
        "\n",
        "    # Process transactions\n",
        "    for transaction in transactions:\n",
        "        for candidate in candidates:\n",
        "            # Single, less redundant subset check\n",
        "            if all(item in transaction for item in candidate):  # Replace redundant subset check\n",
        "                candidate_counts[candidate] += 1\n",
        "\n",
        "    # Inefficient but single-pass filtering\n",
        "    return {itemset: count for itemset, count in candidate_counts.items() if count >= min_support}\n",
        "\n"
      ],
      "metadata": {
        "id": "y2L0y96Ncq_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Generating all frequent itemsets..."
      ],
      "metadata": {
        "id": "2rwhchwWdpmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apriori_gen(itemsets, transactions, min_support):\n",
        "    k = 2\n",
        "    frequent_itemsets = {}\n",
        "\n",
        "    # Add 1-itemsets\n",
        "    frequent_itemsets.update(itemsets)\n",
        "\n",
        "    # Generate k-itemsets\n",
        "    while itemsets:\n",
        "        print(f\"Generating {k}-itemset...\")\n",
        "        candidates = generate_candidates(itemsets.keys(), k)\n",
        "        itemsets = filter_candidates(transactions, candidates, min_support)\n",
        "        frequent_itemsets.update(itemsets)\n",
        "        print(f\"Generated {k}-itemset.\")\n",
        "        k += 1\n",
        "\n",
        "    return frequent_itemsets"
      ],
      "metadata": {
        "id": "NYLVOds_ckIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate association rules from frequent itemsets."
      ],
      "metadata": {
        "id": "OAnzYeQid5IZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_rules(frequent_itemsets, transactions, min_confidence):\n",
        "    \"\"\"Generate association rules from frequent itemsets.\"\"\"\n",
        "    rules = []\n",
        "    for itemset, support_count in frequent_itemsets.items():\n",
        "        if len(itemset) > 1:\n",
        "            subsets = list(find_subsets(itemset))\n",
        "            for subset in subsets:\n",
        "                antecedent = subset\n",
        "                consequent = itemset - subset\n",
        "                if consequent:\n",
        "                    antecedent_support = frequent_itemsets.get(antecedent, 0)\n",
        "                    if antecedent_support > 0:\n",
        "                        confidence = support_count / antecedent_support\n",
        "                        if confidence >= min_confidence:\n",
        "                            rules.append({\n",
        "                                'rule': (set(antecedent), set(consequent)),\n",
        "                                'support': support_count,\n",
        "                                'confidence': confidence\n",
        "                            })\n",
        "    return rules"
      ],
      "metadata": {
        "id": "1yZtXSHrc7vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_subsets(itemset):\n",
        "    \"\"\"Generate all subsets of a given itemset.\"\"\"\n",
        "    subsets = []\n",
        "    for i in range(1, len(itemset)):\n",
        "        subsets.extend(combinations(itemset, i))\n",
        "    return [frozenset(subset) for subset in subsets]"
      ],
      "metadata": {
        "id": "JZT44osdc923"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Function"
      ],
      "metadata": {
        "id": "JbMbQ9jNgbEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apriori(filename, min_support_ratio, min_confidence):\n",
        "    \"\"\"Main function to execute the Apriori algorithm.\"\"\"\n",
        "    print(\"Loading data...\")\n",
        "    transactions = load_data(filename)\n",
        "    num_transactions = len(transactions)\n",
        "    print(f\"Loaded {num_transactions} transactions.\")\n",
        "    # print(transactions)\n",
        "\n",
        "    # Calculate absolute min_support\n",
        "    min_support = int(min_support_ratio * num_transactions)\n",
        "    print(f\"Minimum Support Count: {min_support}\")\n",
        "\n",
        "    print(\"Generating 1-itemsets...\")\n",
        "    one_itemsets = generate_1_itemsets(transactions)\n",
        "    one_itemsets = {itemset: count for itemset, count in one_itemsets.items() if count >= min_support}\n",
        "    print(f\"Total {len(one_itemsets)} 1-itemsets generated\")\n",
        "    # print(one_itemsets)\n",
        "\n",
        "    print(\" 1-itemsets:\")\n",
        "    for itemset, count in one_itemsets.items():\n",
        "        print(f\"{set(itemset)}: {count}\")\n",
        "\n",
        "\n",
        "    print(\"Generating all frequent itemsets...\")\n",
        "    frequent_itemsets = apriori_gen(one_itemsets, transactions, min_support)\n",
        "    print(f\"Total {len(frequent_itemsets)} frequent itemsets generated\")\n",
        "\n",
        "\n",
        "    # print(\"Frequent Itemsets:\")\n",
        "    # for itemset, count in frequent_itemsets.items():\n",
        "    #     print(f\"{set(itemset)}: {count}\")\n",
        "\n",
        "    print(\"\\nGenerating association rules...\")\n",
        "    rules = generate_rules(frequent_itemsets, transactions, min_confidence)\n",
        "    print(f\"Total {len(rules)} association rules generated\")\n",
        "\n",
        "    # for rule in rules:\n",
        "    #     antecedent, consequent = rule['rule']\n",
        "    #     print(f\"Rule: {antecedent} -> {consequent} | Support: {rule['support']} | Confidence: {rule['confidence']:.2f}\")"
      ],
      "metadata": {
        "id": "pQDLnLYkghMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark(datasets, min_support_ratios, min_confidence):\n",
        "    \"\"\"Benchmark Apriori algorithm on multiple datasets and support thresholds.\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for dataset in datasets:\n",
        "        for min_support_ratio in min_support_ratios:\n",
        "            print(f\"\\nDataset: {dataset} | Min Support: {min_support_ratio} | Min Confidence: {min_confidence}\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Execute Apriori\n",
        "            apriori(dataset, min_support_ratio, min_confidence)\n",
        "\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            results.append({\n",
        "                'Dataset': dataset,\n",
        "                'Min Support': min_support_ratio,\n",
        "                'Min Confidence': min_confidence,\n",
        "                'Execution Time': elapsed_time,\n",
        "            })\n",
        "\n",
        "            print(f\"Execution Time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "    # Return results for further analysis or export\n",
        "    return results"
      ],
      "metadata": {
        "id": "A2Uc-iOjfxj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run benchmark\n",
        "results = benchmark(datasets, min_support_ratios, min_confidence)\n",
        "\n",
        "# Export results to a CSV for analysis\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"apriori_benchmark_results.csv\", index=False)\n",
        "print(\"\\nBenchmark results saved to apriori_benchmark_results.csv\")"
      ],
      "metadata": {
        "id": "8UrFSkhmfgnA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}