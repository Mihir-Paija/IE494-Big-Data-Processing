{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwlbNz3IrWK2"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from itertools import chain, combinations\n",
        "import time"
      ],
      "metadata": {
        "id": "hROmgQkQlzZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "OeNPpUPDuIM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc2169cc-29e4-41a9-c059-e1b8e5f1893f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'sc' in globals():\n",
        "    sc.stop()\n",
        "\n",
        "conf = SparkConf().setAppName(\"YAFIM\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "BknJZ5V8uKDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility Functions"
      ],
      "metadata": {
        "id": "0YJrsoSN0wt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Print List"
      ],
      "metadata": {
        "id": "4fANAzaH1nPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def printList(list_a):\n",
        "  for e in list_a:\n",
        "    print (e)"
      ],
      "metadata": {
        "id": "CesVCRqkuL6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG = 1\n",
        "\n",
        "def Dprint(info):\n",
        "    if DEBUG:\n",
        "        print(info)"
      ],
      "metadata": {
        "id": "LPvljOQurKxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate Next Candidates"
      ],
      "metadata": {
        "id": "nKkzr-i406x2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_next_c(f_k, k):\n",
        "    next_c = [var1 | var2 for index, var1 in enumerate(f_k) for var2 in f_k[index + 1:] if\n",
        "              list(var1)[:k - 2] == list(var2)[:k - 2]]\n",
        "    return next_c"
      ],
      "metadata": {
        "id": "uEfBmvHuxERq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filter Candidates for Frequent Itemset"
      ],
      "metadata": {
        "id": "o8tOnXqg0-Yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_f_k(sc, c_k, shared_itemset, sup):\n",
        "    def get_sup(x):\n",
        "        x_sup = len([1 for t in shared_itemset.value if x.issubset(t)])\n",
        "        if x_sup >= sup:\n",
        "            return x, x_sup\n",
        "        else:\n",
        "            return ()\n",
        "\n",
        "    f_k = sc.parallelize(c_k).map(get_sup).filter(lambda x: x).collect()\n",
        "    return f_k"
      ],
      "metadata": {
        "id": "Yvn-pD2uyqYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Find Subsets of Given Itemset"
      ],
      "metadata": {
        "id": "b2Y_qdN61EtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_subsets(itemset):\n",
        "    return [frozenset(subset) for subset in chain.from_iterable(combinations(itemset, r) for r in range(1, len(itemset)))]\n"
      ],
      "metadata": {
        "id": "2NFtyfRwu_h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate Rules"
      ],
      "metadata": {
        "id": "FoDZn2Ay1Hq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_rules(frequent_itemsets, transactions, min_confidence):\n",
        "\n",
        "    frequent_itemsets_dict = {itemset: support_count for itemset, support_count in frequent_itemsets}\n",
        "\n",
        "    rules = []\n",
        "    for itemset, support_count in frequent_itemsets_dict.items():\n",
        "        if len(itemset) > 1:\n",
        "            subsets = list(find_subsets(itemset))\n",
        "            for subset in subsets:\n",
        "                antecedent = frozenset(subset)\n",
        "                consequent = itemset - antecedent\n",
        "                if consequent:\n",
        "                    antecedent_support = frequent_itemsets_dict.get(antecedent, 0)\n",
        "                    if antecedent_support > 0:\n",
        "                        confidence = support_count / antecedent_support\n",
        "                        if confidence >= min_confidence:\n",
        "                            rules.append({\n",
        "                                'rule': (set(antecedent), set(consequent)),\n",
        "                                'support': support_count,\n",
        "                                'confidence': round(confidence, 2)\n",
        "                            })\n",
        "    return rules"
      ],
      "metadata": {
        "id": "_N4FD_svu3DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YAFIM: Apriori Using Spark"
      ],
      "metadata": {
        "id": "4OBmOGDL1SWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ParallelAprioriRunner(sc, data, min_sup, min_confidence):\n",
        "    import time\n",
        "    start = time.time()\n",
        "    # --------- Phase I -----------\n",
        "    # Step 1: Load Data\n",
        "    input = sc.textFile(data)\n",
        "    filtered_input = input.filter(lambda line: line.strip() and not all(char == ',' for char in line.strip()))\n",
        "\n",
        "    # Step 2: Extract Transactions\n",
        "    TransactionRDD = filtered_input.map(lambda line: line.strip().split(\",\"))\n",
        "    transactions = TransactionRDD.map(lambda x: set(map(int, filter(None, x))))  # Transactions as sets\n",
        "    transactions_collected = transactions.collect()  # Collect transactions for broadcasting\n",
        "\n",
        "    # Step 3:\n",
        "    broadcast_transactions = sc.broadcast(transactions_collected)\n",
        "\n",
        "    # Count total transactions\n",
        "    n_samples = len(transactions_collected)\n",
        "    sup = n_samples * min_sup\n",
        "    print(f\"Support threshold: {sup}\")\n",
        "\n",
        "    # Step 4: Initialize Variables\n",
        "    frequent_itemsets = []\n",
        "    k = 1\n",
        "    current_frequent_itemsets = []\n",
        "\n",
        "    #Step 5: Generate Frequent Itemset for k = 1\n",
        "    single_items = transactions.flatMap(lambda x: x).map(lambda x: (frozenset([x]), 1))\n",
        "    single_items_count = single_items.reduceByKey(lambda x, y: x + y)\n",
        "    current_frequent_itemsets = single_items_count.filter(lambda x: x[1] >= sup).collect()\n",
        "    print(f\"Frequent Itemsets for k = 1: \")\n",
        "    printList(current_frequent_itemsets)\n",
        "\n",
        "    frequent_itemsets.extend(current_frequent_itemsets)\n",
        "\n",
        "    # --------- Phase II -----------\n",
        "    # Step 6: Generate Frequent Itemsets for k>=2\n",
        "    print(\"\\nGenerating frequent itemsets...\")\n",
        "    while current_frequent_itemsets:\n",
        "        current_frequent_itemsets_only = [itemset for itemset, _ in current_frequent_itemsets]\n",
        "        candidate_itemsets = [\n",
        "            i.union(j) for i in current_frequent_itemsets_only for j in current_frequent_itemsets_only if len(i.union(j)) == k + 1\n",
        "        ]\n",
        "        candidate_itemsets_rdd = sc.parallelize(candidate_itemsets).distinct()\n",
        "        candidate_counts = candidate_itemsets_rdd.map(\n",
        "            lambda candidate: (\n",
        "                candidate,\n",
        "                sum(1 for transaction in broadcast_transactions.value if candidate <= transaction)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        current_frequent_itemsets = candidate_counts.filter(lambda x: x[1] >= sup).collect()\n",
        "        print(f\"Frequent Itemsets for k = {k}: {len(current_frequent_itemsets)}\")\n",
        "        frequent_itemsets.extend(current_frequent_itemsets)\n",
        "\n",
        "        k += 1\n",
        "\n",
        "\n",
        "    print(f\"Frequent Itemsets Length: {len(frequent_itemsets)}\")\n",
        "    # print(f\"Frequent Itemsets with Frequencies: {frequent_itemsets}\")\n",
        "\n",
        "    #Step 7: Generate Rules\n",
        "    print(\"\\nGenerating association rules...\")\n",
        "    rules = generate_rules(frequent_itemsets, transactions, min_confidence)\n",
        "    print(f\"Total {len(rules)} association rules generated\")\n",
        "    print(f\"Execution Time: {time.time() - start} seconds\")\n",
        "\n",
        "\n",
        "    return frequent_itemsets, rules  # Each itemset is now accompanied by its frequency (count)\n"
      ],
      "metadata": {
        "id": "UQBhZ17-qDRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PUMSB Dataset"
      ],
      "metadata": {
        "id": "fiJBsdqJZDtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pumsb = \"/content/gdrive/MyDrive/Big Data/datasets/pumsb.csv\"\n",
        "min_support = 0.7\n",
        "min_conf = 0.3\n",
        "frequent_itemsets, rules = ParallelAprioriRunner(sc, pumsb, min_support, min_conf)\n",
        "print(f\"----Frequent Itemsets-------\")\n",
        "print(frequent_itemsets)\n",
        "print(f\"\\n----Association Rules-------\")\n",
        "print(rules)"
      ],
      "metadata": {
        "id": "3iuq6zm_Ms6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retail Dataset"
      ],
      "metadata": {
        "id": "uNDg9V-2ZHm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retail = \"/content/gdrive/MyDrive/Big Data/datasets/retail.csv\"\n",
        "min_support = 0.025\n",
        "min_conf = 0.3\n",
        "frequent_itemsets, rules = ParallelAprioriRunner(sc, retail, min_support, min_conf)\n",
        "print(f\"----Frequent Itemsets-------\")\n",
        "print(frequent_itemsets)\n",
        "print(f\"\\n----Association Rules-------\")\n",
        "print(rules)"
      ],
      "metadata": {
        "id": "0C4s1Kvu0IiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mushroom Dataset"
      ],
      "metadata": {
        "id": "3E27SO5rZJg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mushroom = \"/content/gdrive/MyDrive/Big Data/datasets/mushroom.csv\"\n",
        "min_support = 0.3\n",
        "min_conf = 0.3\n",
        "frequent_itemsets, rules = ParallelAprioriRunner(sc, mushroom, min_support, min_conf)\n",
        "print(f\"----Frequent Itemsets-------\")\n",
        "print(frequent_itemsets)\n",
        "print(f\"\\n----Association Rules-------\")\n",
        "print(rules)"
      ],
      "metadata": {
        "id": "P4qhLGt380Sm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}